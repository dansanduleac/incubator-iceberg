{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Iceberg is a new table format for large, slow-moving tabular data. It is designed to improve on the de-facto standard table layout built into Hive, Presto, and Spark. Iceberg Overview Iceberg tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic operation. The table metadata file tracks the table schema, partitioning config, other properties, and snapshots of the table contents. The atomic transitions from one table metadata file to the next provide snapshot isolation. Readers use the latest table state (snapshot) that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. A snapshot is a complete set of data files in the table at some point in time. Snapshots are listed in the metadata file, but the files in a snapshot are stored across a separate set of manifest files. Data files in snapshots are stored in one or more manifest files that contain a row for each data file in the table, its partition data, and its metrics. A snapshot is the union of all files in its manifests. Manifest files can be shared between snapshots to avoid rewriting metadata that is slow-changing. Design benefits This design addresses specific problems with the hive layout: file listing is no longer used to plan jobs and files are created in place without renaming. This also provides improved guarantees and performance: Snapshot isolation : Readers always use a consistent snapshot of the table, without needing to hold a lock. All table updates are atomic. O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls. Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck. Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data. Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning. Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables. Why a new table format? The central metastore can be a scale bottleneck and the file system doesn t and shouldn t provide transactions to isolate concurrent reads and writes. There are several problems with the current format: There is no specification : Implementations don\u2019t handle all cases consistently. For example, bucketing in Hive and Spark use different hash functions and are not compatible. Hive uses a locking scheme to make cross-partition changes safe, but no other implementations use it. Table data is tracked in two stores : a central metastore, for partitions, and the file system, for files. This makes atomic changes to a table\u2019s contents impossible. It also requires job planning to make listing calls that are O(n) with the number of partitions. These listing calls are expensive, especially when using a remote object store. Operations depend on file rename : Most output committers depend on rename operations to implement guarantees and reduce the amount of time tables only have partial data from a write. But rename is not a metadata-only operation in S3 and will copy data. The new S3 committers that use multipart upload make this better, but can\u2019t entirely solve the problem and put a lot of load on the file system during job commit. The current format s dependence on listing and rename cannot be changed, so a new format is needed. Other design goals In addition to changes in how table contents are tracked, Iceberg s design improves a few other areas: Reliable types : Iceberg provides a core set of types, tested to work consistently across all of the supported data formats. Types include date, timestamp, and decimal, as well as nested combinations of map, list, and struct. Schema evolution : Columns are tracked by ID to fully support add, drop, and rename across all columns and nested fields. Hidden partitioning : Partitioning is built into Iceberg as table configuration; it can plan efficient queries without extra partition predicates. Stats-based split filtering : Stats for the columns in each data file are used to eliminate splits before creating tasks, boosting performance for highly selective queries. Metrics : The format includes cost-based optimization metrics stored with data files for better job planning. Unmodified partition data : The Hive layout stores partition data escaped in strings. Iceberg stores partition data without modification. Portable spec : Tables are not tied to Java. Iceberg has a clear specification for other implementations.","title":"About"},{"location":"#iceberg-overview","text":"Iceberg tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic operation. The table metadata file tracks the table schema, partitioning config, other properties, and snapshots of the table contents. The atomic transitions from one table metadata file to the next provide snapshot isolation. Readers use the latest table state (snapshot) that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. A snapshot is a complete set of data files in the table at some point in time. Snapshots are listed in the metadata file, but the files in a snapshot are stored across a separate set of manifest files. Data files in snapshots are stored in one or more manifest files that contain a row for each data file in the table, its partition data, and its metrics. A snapshot is the union of all files in its manifests. Manifest files can be shared between snapshots to avoid rewriting metadata that is slow-changing.","title":"Iceberg Overview"},{"location":"#design-benefits","text":"This design addresses specific problems with the hive layout: file listing is no longer used to plan jobs and files are created in place without renaming. This also provides improved guarantees and performance: Snapshot isolation : Readers always use a consistent snapshot of the table, without needing to hold a lock. All table updates are atomic. O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls. Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck. Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data. Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning. Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables.","title":"Design benefits"},{"location":"#why-a-new-table-format","text":"The central metastore can be a scale bottleneck and the file system doesn t and shouldn t provide transactions to isolate concurrent reads and writes. There are several problems with the current format: There is no specification : Implementations don\u2019t handle all cases consistently. For example, bucketing in Hive and Spark use different hash functions and are not compatible. Hive uses a locking scheme to make cross-partition changes safe, but no other implementations use it. Table data is tracked in two stores : a central metastore, for partitions, and the file system, for files. This makes atomic changes to a table\u2019s contents impossible. It also requires job planning to make listing calls that are O(n) with the number of partitions. These listing calls are expensive, especially when using a remote object store. Operations depend on file rename : Most output committers depend on rename operations to implement guarantees and reduce the amount of time tables only have partial data from a write. But rename is not a metadata-only operation in S3 and will copy data. The new S3 committers that use multipart upload make this better, but can\u2019t entirely solve the problem and put a lot of load on the file system during job commit. The current format s dependence on listing and rename cannot be changed, so a new format is needed.","title":"Why a new table format?"},{"location":"#other-design-goals","text":"In addition to changes in how table contents are tracked, Iceberg s design improves a few other areas: Reliable types : Iceberg provides a core set of types, tested to work consistently across all of the supported data formats. Types include date, timestamp, and decimal, as well as nested combinations of map, list, and struct. Schema evolution : Columns are tracked by ID to fully support add, drop, and rename across all columns and nested fields. Hidden partitioning : Partitioning is built into Iceberg as table configuration; it can plan efficient queries without extra partition predicates. Stats-based split filtering : Stats for the columns in each data file are used to eliminate splits before creating tasks, boosting performance for highly selective queries. Metrics : The format includes cost-based optimization metrics stored with data files for better job planning. Unmodified partition data : The Hive layout stores partition data escaped in strings. Iceberg stores partition data without modification. Portable spec : Tables are not tied to Java. Iceberg has a clear specification for other implementations.","title":"Other design goals"},{"location":"community/","text":"Welcome! Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests. Community discussions happen primarily on the dev mailing list or on specific issues. Issues Issues are tracked in GitHub: View open issues Open a new issue Contributing Iceberg uses Apache s GitHub integration. The code is available at https://github.com/apache/incubator-iceberg The Iceberg community prefers to receive contributions as Github pull requests . View open pull requests Learn about pull requests Mailing Lists Iceberg has three mailing lists: Developers : used for community discussions Subscribe Unsubscribe Archive Commits : distributes commit notifications Subscribe Unsubscribe Archive Private : private PMC list for committer nominations Archive","title":"Community"},{"location":"community/#welcome","text":"Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests. Community discussions happen primarily on the dev mailing list or on specific issues.","title":"Welcome!"},{"location":"community/#issues","text":"Issues are tracked in GitHub: View open issues Open a new issue","title":"Issues"},{"location":"community/#contributing","text":"Iceberg uses Apache s GitHub integration. The code is available at https://github.com/apache/incubator-iceberg The Iceberg community prefers to receive contributions as Github pull requests . View open pull requests Learn about pull requests","title":"Contributing"},{"location":"community/#mailing-lists","text":"Iceberg has three mailing lists: Developers : used for community discussions Subscribe Unsubscribe Archive Commits : distributes commit notifications Subscribe Unsubscribe Archive Private : private PMC list for committer nominations Archive","title":"Mailing Lists"},{"location":"partitioning/","text":"What is partitioning? Partitioning is a way to make queries faster by grouping similar rows together when writing. For example, queries for log entries from a logs table would usually include a time range, like this query for logs between 10 and 12 AM: SELECT level, message FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' Configuring the logs table to partition by the date of event_time will group log events into files with the same event date. Iceberg keeps track of that date and will use it to skip files for other dates that don t have useful data. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries. What does Iceberg do differently? Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning . Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed. Partitioning in Hive To demonstrate the difference, consider how Hive would handle a logs table. In Hive, partitions are explicit and appear as a column, so the logs table would have a column called event_date . When writing, an insert needs to supply the data for the event_date column: INSERT INTO logs PARTITION (event_date) SELECT level, message, event_time, format_time(event_time, 'YYYY-MM-dd') FROM unstructured_log_source Similarly, queries that search through the logs table must have an event_date filter in addition to an event_time filter. SELECT level, count(1) as count FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' AND event_date = '2018-12-01' If the event_date filter were missing, Hive would scan through every file in the table because it doesn t know that the event_time column is related to the event_date column. Problems with Hive partitioning Hive must be given partition values. In the logs example, it doesn t know the relationship bewteen event_time and event_date . This leads to several problems: Hive can t validate partition values it is up to the writer to produce the correct value Using the wrong format, 2018-12-01 instead of 20181201 , produces silently incorrect results, not query failures Using the wrong source column, like processing_time , or time zone also causes incorrect results, not failures It is up to the user to write queries correctly Using the wrong format also leads to silently incorrect results Users that don t understand a table s physical layout get needlessly slow queries Hive can t translate filters automatically Working queries are tied to the table s partitioning scheme, so partitioning configuration cannot be changed without breaking queries Iceberg s hidden partitioning Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date , and keeps track of the relationship. Table partitioning is configured using these relationships. The logs table would be partitioned by date(event_time) and level . Because Iceberg doesn t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn t even see event_date . Most importantly, queries no longer depend on a table s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration.","title":"Partitioning"},{"location":"partitioning/#what-is-partitioning","text":"Partitioning is a way to make queries faster by grouping similar rows together when writing. For example, queries for log entries from a logs table would usually include a time range, like this query for logs between 10 and 12 AM: SELECT level, message FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' Configuring the logs table to partition by the date of event_time will group log events into files with the same event date. Iceberg keeps track of that date and will use it to skip files for other dates that don t have useful data. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries.","title":"What is partitioning?"},{"location":"partitioning/#what-does-iceberg-do-differently","text":"Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning . Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed.","title":"What does Iceberg do differently?"},{"location":"partitioning/#partitioning-in-hive","text":"To demonstrate the difference, consider how Hive would handle a logs table. In Hive, partitions are explicit and appear as a column, so the logs table would have a column called event_date . When writing, an insert needs to supply the data for the event_date column: INSERT INTO logs PARTITION (event_date) SELECT level, message, event_time, format_time(event_time, 'YYYY-MM-dd') FROM unstructured_log_source Similarly, queries that search through the logs table must have an event_date filter in addition to an event_time filter. SELECT level, count(1) as count FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' AND event_date = '2018-12-01' If the event_date filter were missing, Hive would scan through every file in the table because it doesn t know that the event_time column is related to the event_date column.","title":"Partitioning in Hive"},{"location":"partitioning/#problems-with-hive-partitioning","text":"Hive must be given partition values. In the logs example, it doesn t know the relationship bewteen event_time and event_date . This leads to several problems: Hive can t validate partition values it is up to the writer to produce the correct value Using the wrong format, 2018-12-01 instead of 20181201 , produces silently incorrect results, not query failures Using the wrong source column, like processing_time , or time zone also causes incorrect results, not failures It is up to the user to write queries correctly Using the wrong format also leads to silently incorrect results Users that don t understand a table s physical layout get needlessly slow queries Hive can t translate filters automatically Working queries are tied to the table s partitioning scheme, so partitioning configuration cannot be changed without breaking queries","title":"Problems with Hive partitioning"},{"location":"partitioning/#icebergs-hidden-partitioning","text":"Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date , and keeps track of the relationship. Table partitioning is configured using these relationships. The logs table would be partitioned by date(event_time) and level . Because Iceberg doesn t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn t even see event_date . Most importantly, queries no longer depend on a table s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration.","title":"Iceberg's hidden partitioning"}]}